{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIL/xKT8iPjdTRvxvdY0gv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Svyatoslav367948/AI_work/blob/main/AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CatteK_NMDac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51571f55-3ce2-47c8-9ec1-bbc65d3eec25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain>=0.2.0 in /usr/local/lib/python3.12/dist-packages (1.2.7)\n",
            "Requirement already satisfied: langgraph>=0.0.40 in /usr/local/lib/python3.12/dist-packages (1.0.7)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (2.12.5)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.7 in /usr/local/lib/python3.12/dist-packages (from langchain>=0.2.0) (1.2.7)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph>=0.0.40) (2.1.2)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph>=0.0.40) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph>=0.0.40) (0.3.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph>=0.0.40) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0) (2.41.5)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0) (0.4.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->langchain>=0.2.0) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->langchain>=0.2.0) (0.6.5)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->langchain>=0.2.0) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->langchain>=0.2.0) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->langchain>=0.2.0) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->langchain>=0.2.0) (0.14.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph>=0.0.40) (1.12.2)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph>=0.0.40) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph>=0.0.40) (3.11.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph>=0.0.40) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph>=0.0.40) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph>=0.0.40) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph>=0.0.40) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph>=0.0.40) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.7->langchain>=0.2.0) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain>=0.2.0) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain>=0.2.0) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain>=0.2.0) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain>=0.2.0) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain>=0.2.0) (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U \"langchain>=0.2.0\" \"langgraph>=0.0.40\" \"pydantic>=2.0\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from typing import TypedDict, Annotated, List, Dict\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "os.environ[\"MISTRAL_API_KEY\"] = \"d5Udt4PQOdk0uNYBAqDGdIVGSaef6Qna\"\n",
        "\n",
        "PARTICIPANT_NAME_MAIN = \"Бухарев Святослав Андреевич\"\n",
        "\n",
        "llm = ChatMistralAI(\n",
        "    model=\"open-mistral-nemo\",\n",
        "    temperature=0.3,\n",
        "    max_retries=3\n",
        ")\n",
        "\n",
        "MAX_QUESTIONS = 10\n",
        "LOW_SCORE_THRESHOLD = 3\n",
        "\n",
        "class ObserverAnalysis(BaseModel):\n",
        "    score: int = Field(..., ge=1, le=10)\n",
        "    advice: str = Field(...)\n",
        "    topic: str = Field(...)\n",
        "    correct_answer: str = Field(default=\"\")\n",
        "    clarity: int = Field(..., ge=1, le=10)\n",
        "    honesty: int = Field(..., ge=1, le=10)\n",
        "    engagement: int = Field(..., ge=1, le=10)\n",
        "    is_off_topic: bool = Field(default=False)\n",
        "    has_hallucination: bool = Field(default=False)\n",
        "\n",
        "class InterviewState(TypedDict):\n",
        "    history: Annotated[List[Dict[str, str]], \"Сообщения\"]\n",
        "    internal_thoughts: Annotated[List[str], \"Мысли Observer\"]\n",
        "    position: str\n",
        "    grade: str\n",
        "    experience: str\n",
        "    current_difficulty: int\n",
        "    skills_confirmed: List[str]\n",
        "    knowledge_gaps: List[Dict[str, str]]\n",
        "    soft_skills: Dict[str, int]\n",
        "    total_turns: int\n",
        "    participant_name: str\n",
        "    __end__: bool\n",
        "\n",
        "interviewer_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Ты интервьюер на {position} ({grade}, ~{experience} лет).\n",
        "Сложность: {difficulty}/5.\n",
        "\n",
        "Подсказка от Observer: {observer_advice}\n",
        "\n",
        "История:\n",
        "{history}\n",
        "\n",
        "Задай следующий вопрос. Не повторяйся.\n",
        "Если off-topic или ошибка → верни к теме вежливо.\"\"\")\n",
        "\n",
        "observer_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Анализируй ответ кандидата: {user_ans}\n",
        "\n",
        "Контекст: {hist}\n",
        "\n",
        "Шаг за шагом:\n",
        "1. Проверь факты на точность (не галлюцинируй!).\n",
        "2. Оцени релевантность теме вопроса.\n",
        "3. Присвой score 1-10 (10=идеально, 1=провал/off-topic).\n",
        "\n",
        "Примеры:\n",
        "- Ответ 'Python — динамическая типизация' на вопрос о типах: score=10, correct_answer=''\n",
        "- Ответ 'Python — статическая': score=4, correct_answer='динамическая типизация'\n",
        "- 'Не знаю': score=3, correct_answer='[правильный факт]'\n",
        "- Off-topic: score=1, is_off_topic=true\n",
        "\n",
        "Верни structured output.\"\"\")\n",
        "\n",
        "evaluator_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Составь отчёт по итогам интервью.\n",
        "\n",
        "Кандидат: {name} • {position} • {grade} • ~{experience} лет\n",
        "\n",
        "**Вердикт**\n",
        "Grade: {grade}\n",
        "Hiring recommendation: {hiring}\n",
        "Confidence: {confidence}%\n",
        "\n",
        "**Hard Skills (Technical Review)**\n",
        "\n",
        "| Тема                     | Статус             | Правильный ответ / комментарий          |\n",
        "|--------------------------|--------------------|------------------------------------------|\n",
        "{hard_table}\n",
        "\n",
        "**Soft Skills**\n",
        "Clarity:    {clarity}/10\n",
        "Honesty:    {honesty}/10\n",
        "Engagement: {engagement}/10\n",
        "\n",
        "**Roadmap (что подтянуть)**\n",
        "{roadmap}\n",
        "\n",
        "Вопросов: {total_turns}\"\"\")\n",
        "\n",
        "def interviewer_node(state: InterviewState) -> InterviewState:\n",
        "    history_recent = \"\\n\".join(f\"{m['role']}: {m['content']}\" for m in state[\"history\"][-8:])\n",
        "    advice = state[\"internal_thoughts\"][-1] if state[\"internal_thoughts\"] else \"Начни с базового вопроса.\"\n",
        "\n",
        "    question = llm.invoke(interviewer_prompt.invoke({\n",
        "        \"position\": state[\"position\"],\n",
        "        \"grade\": state[\"grade\"],\n",
        "        \"experience\": state[\"experience\"],\n",
        "        \"difficulty\": state[\"current_difficulty\"],\n",
        "        \"observer_advice\": advice,\n",
        "        \"history\": history_recent or \"(начало)\"\n",
        "    })).content.strip()\n",
        "\n",
        "    q_number = state[\"total_turns\"] + 1\n",
        "    print(f\"\\n[Вопрос {q_number}/{MAX_QUESTIONS}] {question}\")\n",
        "\n",
        "    state[\"history\"].append({\"role\": \"interviewer\", \"content\": question})\n",
        "    return state\n",
        "\n",
        "def user_input_node(state: InterviewState) -> InterviewState:\n",
        "    ans = input(\"Your answer (или 'стоп интервью'): \").strip()\n",
        "    state[\"history\"].append({\"role\": \"user\", \"content\": ans})\n",
        "\n",
        "    stop_variants = [\"стоп интервью\", \"стоп\", \"stop\", \"завершить\", \"finish\", \"end\", \"закончить\"]\n",
        "    if any(v in ans.lower() for v in stop_variants):\n",
        "        state[\"__end__\"] = True\n",
        "        print(\"\\n→ Завершение по команде. Генерируем отчёт...\\n\")\n",
        "    return state\n",
        "\n",
        "def observer_node(state: InterviewState) -> InterviewState:\n",
        "    if not state[\"history\"] or state[\"history\"][-1][\"role\"] != \"user\":\n",
        "        return state\n",
        "\n",
        "    user_ans = state[\"history\"][-1][\"content\"]\n",
        "    hist = \"\\n\".join(f\"{m['role']}: {m['content'][:300]}\" for m in state[\"history\"][-10:])\n",
        "\n",
        "    structured_llm = llm.with_structured_output(ObserverAnalysis)\n",
        "\n",
        "    try:\n",
        "        analysis = structured_llm.invoke(\n",
        "            f\"Анализируй ответ кандидата:\\n{user_ans}\\n\\nКонтекст:\\n{hist}\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(\"Ошибка structured output:\", str(e))\n",
        "        analysis = ObserverAnalysis(\n",
        "            score=5, advice=\"Продолжай\", topic=\"неизвестно\",\n",
        "            correct_answer=\"\", clarity=5, honesty=5, engagement=5,\n",
        "            is_off_topic=False, has_hallucination=False\n",
        "        )\n",
        "\n",
        "    #Корректировка score\n",
        "    if analysis.has_hallucination:\n",
        "        analysis.score = max(1, analysis.score - 4)\n",
        "    if analysis.is_off_topic:\n",
        "        analysis.score = max(1, analysis.score - 3)\n",
        "\n",
        "    #Адаптивность\n",
        "    if analysis.score >= 8:\n",
        "        state[\"current_difficulty\"] = min(5, state[\"current_difficulty\"] + 1)\n",
        "    elif analysis.score <= 4:\n",
        "        state[\"current_difficulty\"] = max(1, state[\"current_difficulty\"] - 1)\n",
        "\n",
        "    if analysis.is_off_topic or analysis.has_hallucination:\n",
        "        analysis.advice = \"Верни к теме: 'Пожалуйста, ответьте корректно на вопрос' \" + analysis.advice\n",
        "\n",
        "    topic = analysis.topic.strip()\n",
        "    if topic:\n",
        "        #Если score <= LOW_SCORE_THRESHOLD — обязательно записываем correct_answer\n",
        "        if analysis.score <= LOW_SCORE_THRESHOLD:\n",
        "            if not analysis.correct_answer.strip():\n",
        "                analysis.correct_answer = \"(правильный ответ определён LLM)\"\n",
        "            state[\"knowledge_gaps\"].append({\n",
        "                \"topic\": topic,\n",
        "                \"correct_answer\": analysis.correct_answer\n",
        "            })\n",
        "        elif not analysis.has_hallucination:\n",
        "            if topic not in state[\"skills_confirmed\"]:\n",
        "                state[\"skills_confirmed\"].append(topic)\n",
        "\n",
        "    state[\"soft_skills\"] = {\n",
        "        \"clarity\": analysis.clarity,\n",
        "        \"honesty\": analysis.honesty,\n",
        "        \"engagement\": analysis.engagement\n",
        "    }\n",
        "\n",
        "    thought = (\n",
        "        f\"[Observer]: Пользователь ответил неуверенно (score={analysis.score}), \"\n",
        "        f\"вероятно пробел в теме '{topic or 'текущей теме'}'\\n\"\n",
        "        f\"[Interviewer]: Хорошо, задам уточняющий вопрос по этой же теме.\"\n",
        "    ) if analysis.score < 6 else (\n",
        "        f\"[Observer]: Хороший ответ (score={analysis.score}), знания подтверждены\\n\"\n",
        "        f\"[Interviewer]: Отлично, можно переходить к более сложному вопросу.\"\n",
        "    )\n",
        "\n",
        "    state[\"internal_thoughts\"].append(thought)\n",
        "    state[\"total_turns\"] += 1\n",
        "\n",
        "    print(f\"[hidden] diff={state['current_difficulty']}  score={analysis.score}\")\n",
        "    return state\n",
        "\n",
        "\n",
        "def generate_final_report(state: InterviewState) -> str:\n",
        "    rows = []\n",
        "    for s in state[\"skills_confirmed\"]:\n",
        "        rows.append(f\"| {s:<28} | ✅ Подтверждено       | —\")\n",
        "\n",
        "    for g in state[\"knowledge_gaps\"]:\n",
        "        ca = g.get(\"correct_answer\", \"(не определён)\").strip()\n",
        "        if not ca:\n",
        "            ca = \"(правильный ответ определён LLM)\"\n",
        "        rows.append(f\"| {g['topic']:<28} | ❌ Пробел             | {ca[:80]}{'...' if len(ca)>80 else ''}\")\n",
        "\n",
        "    hard_table = \"\\n\".join(rows) or \"| —                             | —                      | —\"\n",
        "\n",
        "    roadmap = \"\\n\".join(f\"- {g['topic']}\" for g in state[\"knowledge_gaps\"]) or \"Пробелов не выявлено\"\n",
        "\n",
        "    try:\n",
        "        feedback = llm.invoke(evaluator_prompt.invoke({\n",
        "            \"name\": state[\"participant_name\"],\n",
        "            \"position\": state[\"position\"],\n",
        "            \"grade\": state[\"grade\"],\n",
        "            \"experience\": state[\"experience\"],\n",
        "            \"hiring\": \"Hire\" if len(state[\"skills_confirmed\"]) >= len(state[\"knowledge_gaps\"]) else \"No Hire\",\n",
        "            \"confidence\": min(98, 40 + len(state[\"skills_confirmed\"])*12 - len(state[\"knowledge_gaps\"])*15),\n",
        "            \"hard_table\": hard_table,\n",
        "            \"clarity\": state[\"soft_skills\"].get(\"clarity\", 5),\n",
        "            \"honesty\": state[\"soft_skills\"].get(\"honesty\", 5),\n",
        "            \"engagement\": state[\"soft_skills\"].get(\"engagement\", 5),\n",
        "            \"roadmap\": roadmap,\n",
        "            \"total_turns\": state[\"total_turns\"]\n",
        "        })).content\n",
        "\n",
        "        return f\"Интервью завершено ({state['total_turns']} вопросов)\\n\\n{feedback}\"\n",
        "    except Exception as e:\n",
        "        return (\n",
        "            f\"Ошибка генерации отчёта LLM: {str(e)}\\n\\n\"\n",
        "            f\"Подтверждённые навыки: {state['skills_confirmed']}\\n\"\n",
        "            f\"Пробелы (темы для подтягивания): {[g['topic'] for g in state['knowledge_gaps']]}\\n\"\n",
        "            f\"Soft skills: {state['soft_skills']}\\n\"\n",
        "            f\"Всего вопросов: {state['total_turns']}\"\n",
        "        )\n",
        "\n",
        "\n",
        "workflow = StateGraph(InterviewState)\n",
        "\n",
        "workflow.add_node(\"interviewer\", interviewer_node)\n",
        "workflow.add_node(\"user_input\", user_input_node)\n",
        "workflow.add_node(\"observer\", observer_node)\n",
        "\n",
        "workflow.set_entry_point(\"interviewer\")\n",
        "workflow.add_edge(\"interviewer\", \"user_input\")\n",
        "workflow.add_edge(\"user_input\", \"observer\")\n",
        "workflow.add_conditional_edges(\"observer\", should_continue, {\"interviewer\": \"interviewer\", \"end\": END})\n",
        "\n",
        "app = workflow.compile()\n",
        "\n",
        "#ЗАПУСК\n",
        "\n",
        "print(\"=== Multi-Agent Interview Coach (Mistral) ===\\n\")\n",
        "\n",
        "initial_state = {\n",
        "    \"history\": [],\n",
        "    \"internal_thoughts\": [],\n",
        "    \"current_difficulty\": 2,\n",
        "    \"skills_confirmed\": [],\n",
        "    \"knowledge_gaps\": [],\n",
        "    \"soft_skills\": {\"clarity\": 0, \"honesty\": 0, \"engagement\": 0},\n",
        "    \"total_turns\": 0,\n",
        "    \"participant_name\": input(\"Имя кандидата: \") or \"Свят\",\n",
        "    \"position\": input(\"Позиция: \") or \"Python Developer\",\n",
        "    \"grade\": input(\"Грейд: \") or \"Junior\",\n",
        "    \"experience\": input(\"Опыт лет: \") or \"3\",\n",
        "    \"__end__\": False\n",
        "}\n",
        "\n",
        "print(\"\\nНачало интервью...\\n\")\n",
        "\n",
        "try:\n",
        "    final_state = app.invoke(initial_state)\n",
        "except Exception as e:\n",
        "    print(f\"\\nОшибка выполнения графа: {e}\")\n",
        "    final_state = initial_state\n",
        "\n",
        "#Генерируем отчёт ВСЕГДА в конце\n",
        "final_report = generate_final_report(final_state)\n",
        "final_state[\"final_feedback\"] = final_report\n",
        "\n",
        "print(\"\\n\" + \"═\"*70)\n",
        "print(\"          И Н Т Е Р В Ь Ю   З А В Е Р Ш Е Н О\")\n",
        "print(\"═\"*70 + \"\\n\")\n",
        "\n",
        "print(final_report)\n",
        "\n",
        "#ЛОГ\n",
        "\n",
        "turns = []\n",
        "i = 0\n",
        "turn_id = 1\n",
        "while i < len(final_state[\"history\"]):\n",
        "    turn = {\"turn_id\": turn_id}\n",
        "    updated = False\n",
        "\n",
        "    if i < len(final_state[\"history\"]) and final_state[\"history\"][i][\"role\"] == \"interviewer\":\n",
        "        turn[\"agent_visible_message\"] = final_state[\"history\"][i][\"content\"]\n",
        "        i += 1\n",
        "        updated = True\n",
        "\n",
        "    if i < len(final_state[\"history\"]) and final_state[\"history\"][i][\"role\"] == \"user\":\n",
        "        turn[\"user_message\"] = final_state[\"history\"][i][\"content\"]\n",
        "        i += 1\n",
        "        updated = True\n",
        "\n",
        "    if turn_id - 1 < len(final_state[\"internal_thoughts\"]):\n",
        "        turn[\"internal_thoughts\"] = final_state[\"internal_thoughts\"][turn_id - 1]\n",
        "\n",
        "    if updated:\n",
        "        turns.append(turn)\n",
        "        turn_id += 1\n",
        "    else:\n",
        "        i += 1\n",
        "\n",
        "log = {\n",
        "    \"participant_name_main\": PARTICIPANT_NAME_MAIN,\n",
        "    \"participant_name\": final_state[\"participant_name\"],\n",
        "    \"turns\": turns,\n",
        "    \"final_feedback\": final_report  # ← ТОЧНО полный отчёт\n",
        "}\n",
        "\n",
        "with open(\"interview_log.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(log, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\nЛог сохранён в interview_log.json\")\n",
        "from google.colab import files\n",
        "files.download(\"interview_log.json\")"
      ],
      "metadata": {
        "id": "lWhu6-_sHMzW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "7e60f8fb-da98-4466-f8b6-5d86b0074c59"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Multi-Agent Interview Coach (Mistral) ===\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2687844730.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;34m\"soft_skills\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"clarity\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"honesty\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"engagement\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;34m\"total_turns\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0;34m\"participant_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Имя кандидата: \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Свят\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;34m\"position\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Позиция: \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Python Developer\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;34m\"grade\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Грейд: \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Junior\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n5CL8GfFuVQa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}